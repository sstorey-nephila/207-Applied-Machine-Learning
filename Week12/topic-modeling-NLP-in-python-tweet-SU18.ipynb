{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling and Natural Language Processing with Twitter Data\n",
    "\n",
    "##  Jason Anastasopoulos\n",
    "##  July 30, 2018\n",
    "### Email: ljanastas@princeton.edu\n",
    "\n",
    "The code below provides a brief introduction on acquiring Twitter data using the twitter API via Python. For this exercise I will be acquiring Donald Trump's tweets and will try to figure out what the topics his tweets are using the Latent Dirichlet Allocation  Topic Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os,re,csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import twitter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we enter our Twitter credentials. These can be acquired through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Fri Dec 19 19:38:51 +0000 2008\", \"description\": \"Microsoft Visiting Professor @PrincetonCITP\\u25aa\\ufe0fAssistant Professor @UGA_PA_Policy & Political Science \\u25aa\\ufe0f political economy\\u25aa\\ufe0f machine learning\\u25aa\\ufe0f causal inference\", \"favourites_count\": 920, \"followers_count\": 526, \"friends_count\": 625, \"geo_enabled\": true, \"id\": 18249358, \"id_str\": \"18249358\", \"lang\": \"en\", \"listed_count\": 30, \"location\": \"Princeton, NJ\", \"name\": \"Jason Anastasopoulos\", \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/18249358/1522256331\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/953308288044683264/pfRYpPeN_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/953308288044683264/pfRYpPeN_normal.jpg\", \"profile_link_color\": \"F9F7F7\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"jlanastas\", \"status\": {\"created_at\": \"Mon Jul 30 21:16:13 +0000 2018\", \"favorite_count\": 3, \"id\": 1024040992935174144, \"id_str\": \"1024040992935174144\", \"lang\": \"en\", \"quoted_status_id\": 1022890688525029376, \"quoted_status_id_str\": \"1022890688525029376\", \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"text\": \"Hey #causalinference folks. An epic thread  by @laura_tastic on bias in diff-in-diff and matching in different situ\\u2026 https://t.co/t3wvedySu0\", \"truncated\": true}, \"statuses_count\": 823, \"url\": \"https://t.co/fBLr6MzlyV\"}\n"
     ]
    }
   ],
   "source": [
    "api = twitter.Api(consumer_key='mkz0izzVKDRzkrR4GoyN9FStT',\n",
    "                      consumer_secret='4A1YGFEixYmyUNf2idYC33GKCuFoyJkyKpQVXIXCpDedZe0nOt',\n",
    "                      access_token_key='18249358-xZGyGz8sWmQ9oJ1TBsLKEczwtO24aJ0Q4waDbjxAd',\n",
    "                      access_token_secret='uqH7cC5BLS65iuAEPEv4TXEtUZvFD80wH03xkqiB7SP7Y')\n",
    "\n",
    "print api.VerifyCredentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the Twitter API using keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1023267659671109633, u'#Russian bots fill stadiums during the 2016 campaign... https://t.co/dDE6hWD64W')\n",
      "(1021722109943447552, u'I am much more concerned about the Marxists teaching our kids to hate America than Russian bots spreading nonsense on social media')\n",
      "(1021100243687944192, u'#WalkAway is Russian propaganda \\n#WalkAway is Russian propaganda \\n#WalkAway is Russian propaganda \\n#WalkAway is Rus\\u2026 https://t.co/LoBB9PzGpu')\n",
      "(1024058880274440192, u'@PlumRemson Tweets about vaccines from Russian bots... The election is less than 100 days away. This is one of many\\u2026 https://t.co/f2mLzfnwnB')\n",
      "(1024058788066811905, u'RT @SpockResists: \\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\nRussian bots and trolls. They are using \\u274c to say they are shadowbanned \\U0001f644\\U0001f644 but we can all see their tweets, so\\u2026')\n",
      "(1024058635305938944, u'@TheriotChristy @GovMikeHuckabee Sad, I know.\\nBut I lost a shitload of Russian bots a week ago.\\U0001f61e')\n",
      "(1024058231650484224, u'@Friended4Ever Doesn\\u2019t matter. The things about Russian twitter bots is that they only say stupid shit Americans are saying anyways')\n",
      "(1024058178563137537, u'BEWARE the Russian bots. Don\\u2019t fall for it! #RememberInNovember #RegisterAndVOTE #BlueWave2018 https://t.co/KAIwDiZVnM')\n",
      "(1024058073651073026, u'RT @WicksStephen: @SophiaHelwani @tariqnasheed This is Russian deza. The hashtag \"walk away\" is being spread by Russian bots to suppress De\\u2026')\n",
      "(1024057994655526917, u'@deotonic @SeattleTam @SophiaHelwani @lmcameron1313 @realDonaldTrump Actually only a very small few people that I d\\u2026 https://t.co/3dnE3kwVgm')\n",
      "(1024057888493527040, u'@mitchellvii By \\u201cus\\u201d you mean Russian bots?  I hate traitors like you that masquerade behind a fake mask of being \\u201c\\u2026 https://t.co/Fj5Mi4IWib')\n",
      "(1024057861448585216, u'RT @TomthunkitsMind: HELP WANTED: Help me continue to provide you with funnies, education &amp; info with a donation to cover costs and fees. C\\u2026')\n",
      "(1024057772214808576, u'@5ilence_d0g00d @GlenGanaway @Mr_Electrico @jheil Russian bots are so stupid.')\n",
      "(1024057730628247552, u'RT @BrexitCausesBot: Russian bots caused Brexit')\n",
      "(1024057711825248257, u'RT @SpockResists: \\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\u274c\\nRussian bots and trolls. They are using \\u274c to say they are shadowbanned \\U0001f644\\U0001f644 but we can all see their tweets, so\\u2026')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = api.GetSearch(\"russian bots\") # Replace happy with your search\n",
    "for tweet in search:\n",
    "    print(tweet.id, tweet.text)\n",
    "    \n",
    "len(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python twitter library has a lot of cool functions that you can use and learn about through the help() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method GetUserTimeline in module twitter.api:\n",
      "\n",
      "GetUserTimeline(self, user_id=None, screen_name=None, since_id=None, max_id=None, count=None, include_rts=True, trim_user=False, exclude_replies=False) method of twitter.api.Api instance\n",
      "    Fetch the sequence of public Status messages for a single user.\n",
      "    \n",
      "    The twitter.Api instance must be authenticated if the user is private.\n",
      "    \n",
      "    Args:\n",
      "      user_id (int, optional):\n",
      "        Specifies the ID of the user for whom to return the\n",
      "        user_timeline. Helpful for disambiguating when a valid user ID\n",
      "        is also a valid screen name.\n",
      "      screen_name (str, optional):\n",
      "        Specifies the screen name of the user for whom to return the\n",
      "        user_timeline. Helpful for disambiguating when a valid screen\n",
      "        name is also a user ID.\n",
      "      since_id (int, optional):\n",
      "        Returns results with an ID greater than (that is, more recent\n",
      "        than) the specified ID. There are limits to the number of\n",
      "        Tweets which can be accessed through the API. If the limit of\n",
      "        Tweets has occurred since the since_id, the since_id will be\n",
      "        forced to the oldest ID available.\n",
      "      max_id (int, optional):\n",
      "        Returns only statuses with an ID less than (that is, older\n",
      "        than) or equal to the specified ID.\n",
      "      count (int, optional):\n",
      "        Specifies the number of statuses to retrieve. May not be\n",
      "        greater than 200.\n",
      "      include_rts (bool, optional):\n",
      "        If True, the timeline will contain native retweets (if they\n",
      "        exist) in addition to the standard stream of tweets.\n",
      "      trim_user (bool, optional):\n",
      "        If True, statuses will only contain the numerical user ID only.\n",
      "        Otherwise a full user object will be returned for each status.\n",
      "      exclude_replies (bool, optional)\n",
      "        If True, this will prevent replies from appearing in the returned\n",
      "        timeline. Using exclude_replies with the count parameter will mean you\n",
      "        will receive up-to count tweets - this is because the count parameter\n",
      "        retrieves that many tweets before filtering out retweets and replies.\n",
      "        This parameter is only supported for JSON and XML responses.\n",
      "    \n",
      "    Returns:\n",
      "      A sequence of Status instances, one for each message up to count\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(api.GetUserTimeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'...that Open Borders, large scale Crime, and abolishing ICE is good for them, we must get smart and finally do what\\u2026 https://t.co/9pMVWYu2MR',\n",
       " u'Illegal immigration is a top National Security problem. After decades of playing games, with the whole World laughi\\u2026 https://t.co/4PueWN0YWA']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = api.GetUserTimeline(screen_name=\"realDonaldTrump\", count=5000)\n",
    "tweets = [i.AsDict() for i in t]\n",
    "\n",
    "tweettext = [i[\"text\"] for i in tweets]\n",
    "\n",
    "tweettext[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the text data which are all referenda in New Mexico from 2000-2014. Splits referenda using \"####\" demarcation, extracts each bill and places it into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## So far so good not lets clean this up ###\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are pre-processing the text by creating a tokenizer that splits the documents up into tokens (words or phrases), creating a dictionary of stop words and creating a \"stemmer\" which stems the words (ie removing \"-ing\" endings etc. We also remove extraneous \"bill related\" words such as \"propXX_XXXX\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'thank',\n",
       " u'randpaul',\n",
       " u'ye',\n",
       " u'futur',\n",
       " u'great',\n",
       " u'justic',\n",
       " u'suprem',\n",
       " u'court',\n",
       " u'brett',\n",
       " u'kavanaugh',\n",
       " u'vote',\n",
       " u'mean',\n",
       " u'co',\n",
       " u'lg6ixxncxu']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tweettext:\n",
    "    #print \"Processing\",i\n",
    "    # clean and tokenize document string\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    # remove all numbers\n",
    "    tokens = [x for x in tokens if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "    # remove structural words\n",
    "    tokens = [x for x in tokens if len(x) > 1]\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    tokens = [x for x in tokens if 'http' not in x]\n",
    "    tokens = [x for x in tokens if x not in \"_\"]\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "dictionaryall = corpora.Dictionary(texts)\n",
    "\n",
    "corpusall = [dictionaryall.doc2bow(text) for text in texts]\n",
    "\n",
    "texts[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'depart',\n",
       " u'justic',\n",
       " u'urg',\n",
       " u'suprem',\n",
       " u'court',\n",
       " u'least',\n",
       " u'hear',\n",
       " u'driver',\n",
       " u'licens',\n",
       " u'case',\n",
       " u'illeg',\n",
       " u'immi',\n",
       " u'co',\n",
       " u'teayq9in3t']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'.....They can help solve problems with North Korea, Syria, Ukraine, ISIS, Iran and even the coming Arms Race. Bush\\u2026 https://t.co/hrZ6vrJjVC'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweettext[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs tokenization, stop word removal and number removal and places the corpora into a clean list that will be ready for analysis using the Latent Dirichlet Allocation. \n",
    "\n",
    "Notice that there are two sets of texts that are jointly modeled \"commercetexts\" which are the \"business friendly\" Chamber of Commerce bills and \"nmtexts\" which are the New Mexico state referenda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Latent Dirichlet Allocation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodelall = gensim.models.ldamodel.LdaModel(corpusall, num_topics=7, id2word = dictionaryall, passes=20,\n",
    "                                              minimum_probability=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above estimates a 5 topic topic model using Trump's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, u'0.050*co + 0.014*news + 0.012*rt + 0.012*fake + 0.012*russia'), (5, u'0.058*co + 0.012*meet + 0.011*great + 0.008*now + 0.007*union'), (4, u'0.043*co + 0.020*rt + 0.017*will + 0.014*realdonaldtrump + 0.014*state'), (2, u'0.046*co + 0.013*democrat + 0.013*want + 0.011*year + 0.011*border'), (1, u'0.028*co + 0.014*david + 0.011*great + 0.011*state + 0.011*win')]\n"
     ]
    }
   ],
   "source": [
    "# 5 topics from Trump's tweets derived from the top 5 words\n",
    "\n",
    "print(ldamodelall.print_topics(num_topics=5, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints the first 5 topics from the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic 1: \"great\" + 0.012*\"countri\" + 0.012*\"will\" + 0.009*\"meet\"' - Diplomatic Relations\n",
    "- Topic 2: \"better\" + 0.008*\"collus\" + 0.008*\"today\" + 0.006*\"much\"' -  Russia Special Investigation\n",
    "- Topic 3: \"job\" + 0.012*\"trade\" + 0.011*\"thank\" + 0.008*\"year\"' - Economy\n",
    "- Topic 4: 0.015*\"great\" + 0.011*\"north\" + 0.011*\"korea\" + 0.009*\"just\"' - North Korea\n",
    "- Topic 5: 0.015*\"will\" + 0.011*\"state\" + 0.011*\"foxnew\" + 0.009*\"just\"' - Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout Group Exercise\n",
    "\n",
    "1. Using my Twitter credentials collect 5000 tweets from the New York Times (@nytimes) timeline using the GetUserTImeline() function and also collect 5000 tweets from the Fox News timeline (@FoxNews).\n",
    "\n",
    "2. Pre-process these tweets and estimate TWO k = 5 topic topic models for the New York Times tweets and the Fox News Tweets. Label the topics.\n",
    "\n",
    "3. How do topics differ between the Times and Fox News at least according to their twitter accounts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "\n",
    "# Credentials to use\n",
    "api = twitter.Api(consumer_key='mkz0izzVKDRzkrR4GoyN9FStT',\n",
    "                      consumer_secret='4A1YGFEixYmyUNf2idYC33GKCuFoyJkyKpQVXIXCpDedZe0nOt',\n",
    "                      access_token_key='18249358-xZGyGz8sWmQ9oJ1TBsLKEczwtO24aJ0Q4waDbjxAd',\n",
    "                      access_token_secret='uqH7cC5BLS65iuAEPEv4TXEtUZvFD80wH03xkqiB7SP7Y')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
